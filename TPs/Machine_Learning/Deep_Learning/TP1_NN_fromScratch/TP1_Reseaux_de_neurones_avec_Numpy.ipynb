{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b3pjAUEk2LQ"
   },
   "source": [
    "# Construire et entraîner un perceptron multicouche - étape par étape\n",
    "\n",
    "Dans ce TP, vous allez mettre en œuvre l'entraînement d'un réseau de neurones (perceptron multi-couches) à l'aide de la librairie `numpy`. Pour cela nous allons procéder par étapes successives. Dans un premier temps nous allons traiter le cas d'un perceptron monocouche, en commençant par la passe *forward* de prédiction d'une sortie à partir d'une entrée et des paramètres du perceptron, puis en implémentant la passe *backward* de calcul des gradients de la fonction objectif par rapport aux paramètrès. A partir de là, nous pourrons tester l'entraînement à l'aide de la descente de gradient stochastique.\n",
    "\n",
    "Une fois ces étapes achevées, nous pourrons nous atteler à la construction d'un perceptron multicouche, qui consistera pour l'essentiel en la composition de perceptrons monocouches. \n",
    "\n",
    "Dans ce qui suit, nous adoptons les conventions de notation suivantes : \n",
    "\n",
    "- $(x, y)$ désignent un couple donnée/label de la base d'apprentissage ; $\\hat{y}$ désigne quant à lui la prédiction du modèle sur la donnée $x$.\n",
    "\n",
    "- L'indice $i$ indique la $i^{\\text{ème}}$ dimension d'un vecteur.\n",
    "\n",
    "- L'exposant $[l]$ désigne un objet associé à la $l^{\\text{ème}}$ couche.\n",
    "\n",
    "- L'exposant $(k)$ désigne un objet associé au $k^{\\text{ème}}$ exemple. \n",
    "   \n",
    "Exemple:  \n",
    "- $a^{(2)[3]}_5$ indique la 5ème dimension du vecteur d'activation du 2ème exemple d'entraînement (2), de la 3ème couche [3].\n",
    "\n",
    "\n",
    "Commençons par importer tous les modules nécessaires : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6LBs_NLla1a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JZIXefJlXSV"
   },
   "source": [
    "\n",
    "--- \n",
    "\n",
    "## Perceptron monocouche\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azdcz3QV_k-r"
   },
   "source": [
    "### Perceptron monocouche - passe *forward*\n",
    "\n",
    "Un perceptron mono-couche est un modèle liant une couche d'entrée (en vert, qui n'effectue pas d'opération) à une couche de sortie. Les neurones des deux couches sont connectés par des liaisons pondérées (les poids synaptiques) $W_{xy}$, et les neurones de la couche de sortie portent chacun un biais additif $b_y$. Enfin, une fonction d'activation $f$ est appliquée à l'issue de ces opérations pour obtenir la prédiction du réseau $\\hat{y}$. \n",
    "\n",
    "On a donc :\n",
    "\n",
    "$$\\hat{y} = f ( W_{xy} x + b_y )$$ \n",
    "\n",
    "On posera pour la suite :\n",
    "$$ z = W_{xy} x + b_y $$\n",
    "\n",
    "La figure montre une représentation de ces opérations sous forme de réseau de neurones (Figure 1), mais aussi sous une forme fonctionnelle (Figure 2) qui permet de bien visualiser l'ordre des opérations.\n",
    "\n",
    "<img src=\"img/perceptron.png\" height=300>\n",
    "<img src=\"img/perceptron_forward.png\" height=300>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1U4V-MwOatw4axK2u8sJxaasUMl6A3TPo\" height=300> -->\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=14tq-pbbFLvBZU-8LGvgYA71vrWNmSK73\" height=250> -->\n",
    "\n",
    "Notez que les paramètres du perceptron, que nous allons ajuster par un processus d'optimisation, sont donc les poids synaptiques $W_{xy}$ et les biais $b_y$. Par commodité dans le code, nous considérerons également comme un paramètre le choix de la fonction d'activation.\n",
    "\n",
    "**Remarque importante** : on a ici simplifié les dimensions des tenseurs en considérant que les données étaient prédites une à une par le perceptron. En pratique, on traite souvent les données par *batch*, c'est-à-dire que les prédictions sont faites pour plusieurs données simultanément. Ici pour une taille de *batch* de $m$, cela signifie en fait que :\n",
    " \n",
    "$$ x \\in \\mathbb{R}^{4 \\times m} \\quad\\text{et}\\quad y \\in \\mathbb{R}^{5 \\times m}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBtX2euQDSCS"
   },
   "source": [
    "Complétez la fonction `dense_layer_forward` qui calcule la prédiction  d'un perceptron mono-couche pour une entrée $x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGYbWrRfmIwx"
   },
   "outputs": [],
   "source": [
    "def dense_layer_forward(x, Wxy, by, activation):\n",
    "    \"\"\"\n",
    "    Réalise une unique étape forward de la couche dense telle que décrite dans la figure (2)\n",
    "\n",
    "    Arguments:\n",
    "    x -- l'entrée, tableau numpy de dimension (n_x, m).\n",
    "    Wxy -- Matrice de poids multipliant l'entrée, tableau numpy de shape (n_y, n_x)\n",
    "    by -- Biais additif ajouté à la sortie, tableau numpy de dimension (n_y, 1)\n",
    "    activation -- Chaîne de caractère désignant la fonction d'activation choisie : 'linear', 'sigmoid' ou 'relu'\n",
    "\n",
    "    Retourne :\n",
    "    y_pred -- prédiction, tableau numpy de dimension (n_y, m)\n",
    "    cache -- tuple des valeurs utiles pour la passe backward (rétropropagation du gradient), contient (x, z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### A COMPLETER ### \n",
    "    # calcul de z\n",
    "    z = ...\n",
    "    # calcul de la sortie en appliquant la fonction d'activation\n",
    "    if activation == 'relu':\n",
    "        y = ...\n",
    "    elif activation == 'sigmoid':\n",
    "        y = ...\n",
    "    elif activation == 'linear':\n",
    "        y = ...\n",
    "    else:\n",
    "        print(\"Erreur : la fonction d'activation n'est pas implémentée.\")\n",
    "\n",
    "    # sauvegarde du cache pour la passe backward\n",
    "    cache = (x, z)\n",
    "\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCFTHOqD_Tp"
   },
   "source": [
    "Exécutez les lignes suivantes pour vérifier la validité de votre code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6wlVU37on1k",
    "outputId": "b14b0334-704b-473e-933a-8f56b11d2a27"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x_tmp = np.random.randn(3,10)\n",
    "\n",
    "Wxy = np.random.randn(2,3)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "activation = 'relu'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "\n",
    "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
    "print('----------------------------')\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'sigmoid'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])\n",
    "print('----------------------------')\n",
    "\n",
    "activation = 'linear'\n",
    "y_pred_tmp, cache_tmp = dense_layer_forward(x_tmp, Wxy, by, activation)\n",
    "\n",
    "print(\"y_pred[1] =\\n\", y_pred_tmp[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYbiDw8TptiN"
   },
   "source": [
    "**Affichage attendu**: \n",
    "```Python\n",
    "y_pred.shape = \n",
    " (2, 10)\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.         2.11983968 0.88583246 1.39272594 0.         2.92664609\n",
    " 0.         1.47890228 0.         0.04725575]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [0.10851642 0.89281659 0.70802939 0.80102707 0.21934644 0.94914804\n",
    " 0.24545321 0.81440672 0.48495927 0.51181174]\n",
    "----------------------------\n",
    "y_pred[1] =\n",
    " [-2.10598556  2.11983968  0.88583246  1.39272594 -1.26947904  2.92664609\n",
    " -1.12301093  1.47890228 -0.06018107  0.04725575]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dense_layer_forward.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GypgZ8jBqooR"
   },
   "source": [
    "### Perceptron monocouche - passe *backward*\n",
    "\n",
    "Dans les librairies d'apprentissage profond actuelles, il suffit d'implémenter la passe *forward*, et la passe *backward* est réalisée automatiquement, avec le calcul des gradients (différentiation automatique) et la mise à jour des paramètres. Il est cependant intéressant de comprendre comment fonctionne la passe *backward*, en l'implémentant sur un exemple simple.\n",
    "\n",
    "\n",
    "<img src=\"img/perceptron_retropropagation.png\" height=250>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1bIk-7GppJzkP2HNJ9RMvhjPoBNuNX-yU\" height=250> -->\n",
    "\n",
    "\n",
    "Il faut calculer les dérivées par rapport à la fonction de perte pour ensuite mettre à jour les paramètres du réseau. Les équations de rétropropagation sont données ci-dessous (c'est un bon exercice que de les calculer à la main). \n",
    "\n",
    "\\begin{align}\n",
    "\\displaystyle  {dW_{xy}} &~=~ \\frac{\\partial J}{\\partial W_{xy}} ~=~ \\ldots ~=~ dz \\cdot x^{T}\\tag{1} \\\\[8pt]\n",
    "\\displaystyle db_{y} &~=~ \\frac{\\partial J}{\\partial b_y} ~=~ \\ldots ~=~ \\sum_{batch}dz\\tag{2} \\\\[8pt]\n",
    "\\displaystyle dx &~=~ \\frac{\\partial J}{\\partial x} ~=~ \\ldots ~=~ { W_{xy}}^T \\cdot dz \\tag{3}  \\\\[8pt]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Ici, $\\cdot$ indique une multiplication matricielle tandis que $\\times$ indique une multiplication élément par élément. Par ailleurs :\n",
    "* $\\texttt{dz}$ désigne $\\frac{\\partial J}{\\partial z}=\\frac{\\partial J}{\\partial \\hat{y}}\\times\\frac{\\partial \\hat{y}}{\\partial z}$, \n",
    "* $\\texttt{dWxy}$ désigne $\\frac{\\partial J}{\\partial W_{xy}}$, \n",
    "* $\\texttt{dby}$ désigne $\\frac{\\partial J}{\\partial b_y}$, et \n",
    "* $\\texttt{dx}$ désigne $\\frac{\\partial J}{\\partial x}$.\n",
    "\n",
    "Ces noms ont été choisis pour être utilisables dans le code. On notera également $\\texttt{dy_dz}$ pour $\\frac{\\partial\\hat{y}}{\\partial z}$ et $\\texttt{dy_hat}$ pour $\\frac{\\partial J}{\\partial\\hat{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dense_layer_backward.py\n",
    "def dense_layer_backward(dy_hat, Wxy, by, activation, cache):\n",
    "    \"\"\"\n",
    "    Implémente la passe backward de la couche dense.\n",
    "\n",
    "    Arguments :\n",
    "    dy_hat -- Gradient de la perte J par rapport à la sortie ŷ\n",
    "    cache -- dictionnaire python contenant des variables utiles (issu de dense_layer_forward())\n",
    "\n",
    "    Retourne :\n",
    "    gradients -- dictionnaire python contenant les gradients suivants :\n",
    "                        dx -- Gradients par rapport aux entrées, de dimension (n_x, m)\n",
    "                        dby -- Gradients par rapport aux biais, de dimension (n_y, 1)\n",
    "                        dWxy -- Gradients par rapport aux poids synaptiques Wxy, de dimension (n_y, n_x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Récupérer le cache\n",
    "    (x, z) = cache\n",
    "\n",
    "    ### A COMPLETER ###   \n",
    "    # calcul de la sortie en appliquant l'activation\n",
    "    # dy_dz -- Gradient de la sortie ŷ par rapport à l'état caché z\n",
    "    if activation == 'relu':\n",
    "        dy_dz = np.where(z > 0, 1, 0)\n",
    "    elif activation == 'sigmoid':\n",
    "        dy_dz = np.exp(-z)/np.square(1 + np.exp(-z))\n",
    "    else: # Activation linéaire\n",
    "        dy_dz = np.ones(dy_hat.shape)\n",
    "\n",
    "        \n",
    "    # calculer le gradient de la perte par rapport à x\n",
    "    dx = np.matmul(np.transpose(Wxy), dy_hat*dy_dz)\n",
    "\n",
    "    # calculer le gradient de la perte par rapport à Wxy\n",
    "    dWxy = np.matmul(dy_hat*dy_dz, np.transpose(x))\n",
    "\n",
    "    # calculer le gradient de la perte par rapport à by \n",
    "    dby = np.sum(dy_hat*dy_dz, axis=1, keepdims=True)\n",
    "\n",
    "    ### FIN ###\n",
    "    \n",
    "    # Stocker les gradients dans un dictionnaire\n",
    "    gradients = {\"dx\": dx, \"dby\": dby, \"dWxy\": dWxy}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5KeDgyO-ZPJ"
   },
   "source": [
    "On peut maintenant créer une classe `DenseLayer`, qui comprend en attribut toutes les informations nécessaires à la description d'une couche dense, c'est-à-dire : \n",
    "\n",
    "*   Le nombre de neurones en entrée de la couche dense ($\\texttt{input_size}$)\n",
    "*   Le nombre de neurones en sortie de la couche dense ($\\texttt{output_size}$)\n",
    "*   La fonction d'activation choisie sur cette couche ($\\texttt{activation}$)\n",
    "*   Les poids synaptiques de la couche dense, stockés dans une matrice de taille ($\\texttt{out_size}$, $\\texttt{input_size}$) : $\\texttt{Wxy}$\n",
    "*   Les biais de la couche dense, stockés dans un vecteur de taille ($\\texttt{output_size}$, 1) : $\\texttt{b}y$\n",
    "\n",
    "On ajoute également un attribut cache qui permettra de stocker les entrées de la couche dense ($\\texttt{x}$) ainsi que les calculs intermédiaires ($\\texttt{z}$) réalisés lors de la passe *forward*, afin d'être réutilisés pour la basse *backward*.\n",
    "\n",
    "A vous de compléter les 4 jalons suivants : \n",
    "\n",
    "*   **L'initialisation des paramètres** $\\texttt{Wxy}$ et $\\texttt{by}$ : $\\texttt{Wxy}$ doit être positionnée suivant [l'initialisation de Glorot](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform), et $\\texttt{by}$ est initialisée par un vecteur de zéros.\n",
    "*   **La fonction `forward`**, qui consiste simplement en un appel de la fonction `dense_layer_forward` implémentée précédemment.\n",
    "*   **La fonction `backward`**, qui consiste simplement en un appel de la fonction `dense_layer_backward` implémentée précédemment.\n",
    "*   Et enfin **la fonction `update_parameters`** qui applique la mise à jour de la descente de gradient en fonction d'un taux d'apprentissage (learning_rate) et des gradients calculés dans la passe *forward*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/DenseLayer.py\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "        self.cache = None  # Le cache sera mis à jour lors de la passe forward\n",
    "        \n",
    "        ### A COMPLETER ###\n",
    "        limit = math.sqrt(6 / (input_size + output_size))\n",
    "        self.Wxy = np.random.uniform(low=-limit, high=limit, size=(output_size, input_size)) # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        y, cache = dense_layer_forward(x_batch, self.Wxy, self.by, self.activation)\n",
    "        self.cache = cache\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        return dense_layer_backward(dy, self.Wxy, self.by, self.activation, self.cache)\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        self.Wxy -= learning_rate * gradients[\"dWxy\"]\n",
    "        self.by  -= learning_rate * gradients[\"dby\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GlEB8K3Lani"
   },
   "source": [
    "### Entraînement par descente de gradient stochastique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMcQzlskdI1"
   },
   "source": [
    "Pour entraîner notre modèle, nous devons mettre en place un optimiseur. Nous implémenterons la descente de gradient stochastique avec mini-batch. Il nous faut cependant au préalable implanter la fonction de coût que nous utiliserons pour évaluer la qualité de nos prédictions. \n",
    "\n",
    "Pour le moment, nous allons nous contenter d'une erreur quadratique moyenne, qui associée à une fonction d'activation linéaire (l'identité) permet de résoudre les problèmes de régression. \n",
    "\n",
    "La fonction de coût prend en entrée deux paramètres : la vérité-terrain $\\texttt{y_true}$ et la prédiction du modèle $\\texttt{y_pred}$ . Ces deux matrices sont de dimension $\\texttt{bs$\\,\\times\\,$output_size}$. La fonction retourne deux grandeurs : $\\texttt{loss}$ qui correspond à l'erreur quadratique moyenne des prédictions par rapport aux vérités-terrains, et $\\texttt{grad}$ au gradient de l'erreur quadratique moyenne par rapport aux prédictions. Autrement dit : \n",
    "$$ \\texttt{grad} = \\frac{\\partial J_{mb}}{\\partial \\hat{y}}$$\n",
    "\n",
    "où $\\hat{y}$ correspond à $\\texttt{y_pred}$, et $J_{mb}$ à la fonction objectif calculée sur un mini-batch $mb$ de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/mean_square_error.py\n",
    "def mean_square_error(y_true, y_pred):\n",
    "    loss = np.mean(np.square(y_true - y_pred))\n",
    "    grad = -2 * (y_true - y_pred) / y_true.shape[0]\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w2XnUBj2n-Df"
   },
   "source": [
    "La descente de gradient stochastique prend en entrée les paramètres suivants :  \n",
    "*    $\\texttt{x_train}$ et $\\texttt{y_train}$ respectivement les données et labels de l'ensemble d'apprentissage (que l'on suppose de taille $N$).\n",
    "*    $\\texttt{model}$ une instance du modèle que l'on veut entraîner (qui doit implanter les 3 fonctions vues précédemment $\\texttt{forward}$, $\\texttt{backward}$ et $\\texttt{update_parameters}$).\n",
    "*    $\\texttt{loss_function}$ peut prendre deux valeurs : '$\\texttt{mse}$' (erreur quadratique moyenne) ou '$\\texttt{bce}$' (entropie croisée binaire, que nous implémenterons par la suite).\n",
    "*    $\\texttt{learning_rate}$ le taux d'apprentissage choisi pour la descente de gradient.\n",
    "*    $\\texttt{epochs}$ le nombre de parcours complets de l'ensemble d'apprentissage que l'on veut réaliser.\n",
    "*    $\\texttt{batch_size}$ la taille de mini-batch désirée pour la descente de gradient stochastique. \n",
    "\n",
    "L'algorithme à implémenter est rappelé ci-dessous :       \n",
    "```\n",
    "N_batch = floor(N/batch_size)\n",
    "\n",
    "Répéter epochs fois\n",
    "\n",
    "  Pour b de 1 à N_batch Faire\n",
    "\n",
    "    Sélectionner les données x_train_batch et labels y_train_batch du b-ème mini-batch\n",
    "    Calculer la prédiction y_pred_batch du modèle pour ce mini-batch\n",
    "    Calculer la perte batch_loss et le gradient de la perte batch_grad par rapport aux prédictions sur ce mini-batch\n",
    "    Calculer les gradients de la perte par rapport à chaque paramètre du modèle\n",
    "    Mettre à jour les paramètres du modèle \n",
    "\n",
    "  Fin Pour\n",
    "\n",
    "Fin Répéter\n",
    "\n",
    "```\n",
    "Deux remarques additionnelles :    \n",
    "1. A chaque *epoch*, les *mini-batches* doivent être différents (les données doivent être réparties dans différents *mini-batches*).\n",
    "2. Il est intéressant de calculer (et d'afficher !) la perte moyennée sur l'ensemble d'apprentissage à chaque *epoch*. Pour cela, on peut accumuler les pertes de chaque *mini-batch* sur une *epoch* et diviser l'ensemble par le nombre de *mini-batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/SGD.py\n",
    "def SGD(x_train, y_train, model, loss_function, learning_rate, epochs, batch_size):\n",
    "    # Nombre de batches par epoch\n",
    "    nb_batches = math.floor(x_train.shape[0] / batch_size)\n",
    "\n",
    "    # Pour gérer le tirage aléatoire des batches parmi les données d'entraînement, génération et permutation des indices\n",
    "    indices = np.arange(x_train.shape[0])\n",
    "    indices = np.random.permutation(indices)\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for b in range(nb_batches):\n",
    "\n",
    "            # Sélection des données du batch courant\n",
    "            x_train_batch = x_train[indices[b*batch_size:(b+1)*batch_size]]\n",
    "            y_train_batch = y_train[indices[b*batch_size:(b+1)*batch_size]]\n",
    "\n",
    "            # Prédiction du modèle pour le batch courant\n",
    "            y_pred_batch = model.forward(np.transpose(x_train_batch))\n",
    "\n",
    "            # Calcul de la perte et des gradients sur le batch courant\n",
    "            if loss_function == 'mse':\n",
    "                batch_loss, batch_gradients = mean_square_error(y_train_batch, y_pred_batch)\n",
    "            elif loss_function == 'bce':\n",
    "                batch_loss, batch_gradients = binary_cross_entropy(y_train_batch, y_pred_batch)\n",
    "\n",
    "            running_loss += batch_loss \n",
    "\n",
    "            # Calcul du gradient de la perte par rapport aux paramètres du modèle\n",
    "            param_updates = model.backward(batch_gradients)\n",
    "\n",
    "            # Mise à jour des paramètres du modèle\n",
    "            model.update_parameters(param_updates, learning_rate)\n",
    "\n",
    "        print(\"Epoch %4d : Loss : %.4f\" % (e, float(running_loss/nb_batches)))\n",
    "\n",
    "        # Nouvelle permutation des données pour la prochaine epoch\n",
    "        indices = np.random.permutation(indices)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bybDhHivjXq"
   },
   "source": [
    "### Test sur un problème de régression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7q44eS0vrrZ"
   },
   "source": [
    "Le bloc de code suivant permet de générer et d'afficher des ensembles de données d'apprentissage et de test pour un problème de régression linéaire classique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/regression_data.py\n",
    "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.', label='Ensemble d\\'apprentissage')\n",
    "plt.plot(x_test, y_test, 'r+', label='Ensemble de test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/regression_train.py\n",
    "### A COMPLETER ###\n",
    "\n",
    "model = DenseLayer(1, 1, 'linear')\n",
    "model = SGD(x_train, y_train, model, 'mse', 0.1, 10, 20)\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.', label='Ensemble d\\'apprentissage')\n",
    "plt.plot(x_test, y_test, 'r+', label='Ensemble de test')\n",
    "\n",
    "x_gen = np.expand_dims(np.linspace(-3, 3, 10), 1)\n",
    "y_gen = np.transpose(model.forward(np.transpose(x_gen)))\n",
    "\n",
    "plt.plot(x_gen, y_gen, 'g-', label='Prédiction du modèle')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA9-6PqLwff4"
   },
   "source": [
    "### Test sur un problème de classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9AHAgGBwjro"
   },
   "source": [
    "Afin de pouvoir tester notre perceptron mono-couche sur un problème de classification binaire (i.e. effectuer une régression logistique), il est d'abord nécessaire d'implémenter l'entropie croisée binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/binary_cross_entropy.py\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    loss = np.mean(- y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "    grad =  (- y_true / y_pred + (1 - y_true) / (1 - y_pred))/y_true.shape[0]\n",
    "\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_data_1.py\n",
    "x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_train_1.py\n",
    "# 1- Initialisation des paramètres du modèle\n",
    "\n",
    "model = DenseLayer(2, 1, 'sigmoid')\n",
    "model = SGD(x_train, y_train, model, 'bce', 0.3, 50, 20)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "x_gen = np.linspace(-6, 2, 10)\n",
    "y_gen = -model.Wxy[0,0]*x_gen/model.Wxy[0,1] - model.by[0,0]/model.Wxy[0,1]\n",
    "\n",
    "plt.plot(x_gen, y_gen, 'g-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur un problème de classification binaire plus complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_data_2.py\n",
    "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrire une fonction qui, en chaque point d'une grille adaptée à notre problème permet de visualiser la probabilité d'appartenance à la catégorie considérée (à la manière de [playground.tensorflow.org](http://playground.tensorflow.org/)) ainsi que les différents points de l'ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/print_decision_boundaries.py\n",
    "def print_decision_boundaries(model, x, y):\n",
    "    # make these smaller to increase the resolution\n",
    "    dx, dy = 0.1, 0.1\n",
    "\n",
    "    # generate 2 2d grids for the x & y bounds\n",
    "    y_grid, x_grid = np.mgrid[slice(-4, 4 + dy, dy), slice(-4, 4 + dx, dx)]\n",
    "\n",
    "    x_gen = np.concatenate((np.expand_dims(np.reshape(y_grid, (-1)),1),np.expand_dims(np.reshape(x_grid, (-1)),1)), axis=1)\n",
    "    z_gen = model.forward(np.transpose(x_gen)).reshape(x_grid.shape)\n",
    "\n",
    "    z_min, z_max = 0, 1\n",
    "\n",
    "    c = plt.pcolor(x_grid, y_grid, z_gen, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "    plt.colorbar(c)\n",
    "    plt.plot(x[y==0,0], x[y==0,1], 'r.')\n",
    "    plt.plot(x[y==1,0], x[y==1,1], 'b.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_train_2.py\n",
    "# 1- Initialisation des paramètres du modèle\n",
    "\n",
    "model = DenseLayer(2, 1, 'sigmoid')\n",
    "model = SGD(x_train, y_train, model, 'bce', 0.3, 50, 20)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "x_gen = np.linspace(-6, 2, 10)\n",
    "y_gen = -model.Wxy[0,0]*x_gen/model.Wxy[0,1] - model.by[0,0]/model.Wxy[0,1]\n",
    "\n",
    "plt.plot(x_gen, y_gen, 'g-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiGyXLvum0uI"
   },
   "source": [
    "---\n",
    "\n",
    "## Perceptron multicouche\n",
    "\n",
    "<!-- Nous allons à présent construire un perceptron multi-couches en se basant sur le perceptron mono-couche précédement développé. L'idée est d'accoler des perceptrons mono-couche en utilisant la sortie de la couche précédente comme entrée de la couche courante.\n",
    "\n",
    "En remarquant cela on peut créer une classe `MultiLayerPerceptron`. En particulier, la fonction `add_layer` permet d'ajouter au MLP le *layer* qu'elle prend en entrée. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir du perceptron mono-couche créé précédemment, nous pouvons maintenant implémenter un perceptron multi-couches, qui est un véritable réseau de neurones dans la mesure où il met en jeu plusieurs couches de neurones successives. Concrètement, le perceptron multi-couches est une composition de perceptron monocouches, chacun prenant en entrée l'activation de sortie de la couche précédente. Prenons l'exemple ci-dessous : \n",
    "\n",
    "\n",
    "<img src=\"img/mlp_simple.png\" height=300>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1YV4j3BKa7X93JbzZStTO6GcPS7yL3hOx\" height=250> -->\n",
    "\n",
    "\n",
    "Ce perceptron multi-couches est la composition de deux perceptrons monocouches, le premier liant deux neurons d'entrée à deux neurones de sortie, et le second deux neurones d'entrée à un neurone de sortie.\n",
    "\n",
    "\n",
    "<img src=\"img/mlp_forward.png\" height=250>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=14THL4RfdhfF8In6TPxmVKofR1RHHZSzx\" height=250> -->\n",
    "\n",
    "\n",
    "Voici comment nous l'implémenterons : le perceptron multi-couches consiste simplement en une liste de perceptron monocouches (`DenseLayer`). A l'initialisation, le perceptron multi-couches est une liste vide, dans laquelle il est possible d'ajouter des couches denses (fonction `add_layer()`). \n",
    "\n",
    "```python\n",
    "model = MultiLayerPerceptron()\n",
    "model.add_layer(DenseLayer(2, 2, 'relu'))\n",
    "model.add_layer(DenseLayer(2, 1, 'sigmoid'))\n",
    "```\n",
    "\n",
    "La fonction `forward()` du perceptron multi-couches consiste en le calcul successif de la sortie des couches denses. Chaque couche dense effectue une prédiction sur la sortie de la couche dense précédente.\n",
    "\n",
    "La fonction `backward()` implémente l'algorithme de rétro-propagation du gradient. Les gradients des paramètres de la dernière couche sont calculés en premier, et sont utilisés pour calculer les gradients de la couche précédente, comme illustré sur cette figure.\n",
    "\n",
    "\n",
    "<img src=\"img/mlp_retropropagation.png\" height=250>\n",
    "\n",
    "<!-- <img src=\"https://drive.google.com/uc?id=1E-2tCxPJqn7yLPsUt-jPuRn8krl03MRI\" height=250> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/MultiLayerPerceptron.py\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        \n",
    "    def forward(self, x_batch):\n",
    "        output_l = x_batch\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            # La sortie de la couche précédente est passée en entrée de la couche courante\n",
    "            input_l = output_l\n",
    "            output_l = self.layers[i].forward(input_l)  \n",
    "      \n",
    "        # La sortie de la dernière couche est la sortie finale du réseau\n",
    "        y = output_l\n",
    "\n",
    "        return y\n",
    "     \n",
    "        \n",
    "    def backward(self, dy):\n",
    "        gradients = []\n",
    "      \n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            # La sortie de la couche précédente est passée en entrée de la couche courante\n",
    "            layer_gradients = self.layers[i].backward(dy)  \n",
    "            gradients.append(layer_gradients) \n",
    "            dy = layer_gradients[\"dx\"]\n",
    "      \n",
    "        gradients.reverse()\n",
    "        return gradients\n",
    "\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].update_parameters(gradients[i], learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test sur un problème de classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez votre algorithme sur le jeu de données ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "pijGm1ipwrAw",
    "outputId": "237f56b0-b4da-4006-8e3a-61bf5a19e45c"
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'r.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'b.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'r+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'b+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_train_3.py\n",
    "model = MultiLayerPerceptron()\n",
    "model.add_layer(DenseLayer(2, 10, 'relu'))\n",
    "model.add_layer(DenseLayer(10, 20, 'relu'))\n",
    "model.add_layer(DenseLayer(20, 10, 'relu'))\n",
    "model.add_layer(DenseLayer(10, 10, 'relu'))\n",
    "model.add_layer(DenseLayer(10, 1, 'sigmoid'))\n",
    "\n",
    "model = SGD(x_train, y_train, model, 'bce', 0.1, 60, 20)\n",
    "\n",
    "print_decision_boundaries(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_train_4.py\n",
    "model = MultiLayerPerceptron()\n",
    "model.add_layer(DenseLayer(2, 10, 'relu'))\n",
    "model.add_layer(DenseLayer(10, 1, 'sigmoid'))\n",
    "\n",
    "model = SGD(x, y, model, 'bce', 0.1, 75, 20)\n",
    "\n",
    "print_decision_boundaries(model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron mono- et multicouche en `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayer de retrouver les résultats précédents (régression et classification binaire) en utilisant la librairie [**`keras`**](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron monocouche : Régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_regression(n_samples=250, n_features=1, n_targets=1, random_state=1, noise=10)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.', label='Ensemble d\\'apprentissage')\n",
    "plt.plot(x_test, y_test, 'r+', label='Ensemble de test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/regression_keras.py\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add( Dense(units=1, input_dim=1, activation='linear') )\n",
    "\n",
    "# Configure the model and start training\n",
    "sgd = optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "model.fit(x_train, y_train, verbose=2, epochs=10, batch_size=20)\n",
    "\n",
    "# --- #\n",
    "\n",
    "plt.plot(x_train, y_train, 'b.', label='Ensemble d\\'apprentissage')\n",
    "plt.plot(x_test, y_test, 'r+', label='Ensemble de test')\n",
    "\n",
    "x_gen = np.linspace(-3, 3, 10)\n",
    "y_gen = model.predict(x_gen)\n",
    "\n",
    "plt.plot(x_gen[:], y_gen[:], 'g-', label='Prédiction du modèle')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron multicouche : Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 2\n",
    "x, y = datasets.make_blobs(n_samples=250, n_features=2, centers=2, center_box=(- 3, 3), random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "#plt.scatter(x_train[:,0], x_train[:,1], c=y_train, s=8)\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'b.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_keras_1.py\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add( Dense(units=1, input_dim=2, activation='sigmoid') )\n",
    "\n",
    "# Configure the model and start training\n",
    "sgd = optimizers.SGD(learning_rate=0.3)\n",
    "model.compile(optimizer='sgd', loss='BinaryCrossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, verbose=2, epochs=50, batch_size=20)\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron multicouche : Classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = datasets.make_gaussian_quantiles(n_samples=250, n_features=2, n_classes=2, random_state=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1/10, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/9, random_state=1)\n",
    "\n",
    "plt.plot(x_train[y_train==0,0], x_train[y_train==0,1], 'r.')\n",
    "plt.plot(x_train[y_train==1,0], x_train[y_train==1,1], 'b.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'r+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'b+')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification_keras_2.py\n",
    "input_dim = x_train.shape[1]\n",
    "print(f'Feature dim: {input_dim}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=10, input_dim=input_dim, activation='relu'))\n",
    "model.add(Dense(units=1, input_dim=10, activation='sigmoid'))\n",
    "\n",
    "# Configure the model and start training\n",
    "sgd = optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(loss='BinaryCrossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=60, batch_size=20, verbose=1)\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP Réseaux de Neurones avec Numpy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
