{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4gemxU6eYk-"
   },
   "source": [
    "# Transformers pour la classification de texte\n",
    "\n",
    "L'objectif de ce TP est d'implémenter une version simplifiée d'un Transformer pour résoudre un problème de classification de texte.\n",
    "\n",
    "Nous utiliserons comme exemple illustratif une base de données présente dans la librairie ```Keras``` consistant en des critiques de films postées sur le site IMDB, accompagnées d'une note qui a été binarisée pour révéler le caracète positif, ou négatif, de la critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ypGxrmyTeYlE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 14:03:56.367701: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-20 14:03:56.544311: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-20 14:03:57.475962: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jBnJa2VeYlF"
   },
   "source": [
    "## Implémentation d'un bloc de base de Transformer\n",
    "\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1w1CyLROPq-EWMd-Spr6wR596QEx1KpNa\"> </center>\n",
    "<caption><center> Figure 1: Schéma de l'architecture de GPT 1</center></caption>\n",
    "\n",
    "La figure ci-dessus présente l'architecture de GPT-1. Le bloc de base d'un Transformer est composé d'un bloc de *Self-Attention*, d'une couche de ```Layer Normalization``` (similaire à la ```Batch Normalization```), d'une couche dense et enfin d'une nouvelle couche de ```Layer Normalization```.\n",
    "\n",
    "Pour implémenter la *Self-Attention*, vous pouvez utiliser la fonction ```layers.MultiHeadAttention``` (à vous de regarder quels en sont les paramètres dans la documentation).\n",
    "\n",
    "**Rappel**: Une couche d'Attention *Multi-Head*  se présente sous la forme ci-dessous à gauche, avec le mécanisme d'attention détaillé à droite :\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UTozEHtsZ3xy61XJqn_Eug-7mn7bFp9m\">\n",
    "<img src=\"https://drive.google.com/uc?id=1aTttpp1OOasVVZAi3lWwosh68VnBjQnz\">\n",
    "</center>\n",
    "\n",
    "D'après vous, combien de paramètres comporte une couche d'attention à 2 têtes, pour un *Embedding* de dimension 32 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuK3YnbLeYlF"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    # embed_dim désigne la dimension des embeddings maintenus à travers les différentes couches,\n",
    "    # et num_heads le nombre de têtes de la couche d'attention.\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # Définition des différentes couches qui composent le bloc\n",
    "        # Couche d'attention\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "        # Première couche de Layer Normalization\n",
    "        self.layernorm1 = layers.Normalization()\n",
    "        # Couche Dense (Feed-Forward)\n",
    "        self.ffn = layers.Dense(embed_dim, activation = 'relu')\n",
    "        # Deuxième couche de normalisation\n",
    "        self.layernorm2 = layers.Normalization()\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Application des couches successives aux entrées\n",
    "        x = self.att(inputs, inputs)\n",
    "        out1 = selflayernorm1(x + self)\n",
    "        out2 = self.ffn(out1)\n",
    "        out3 = selflayernorm2(out1 + out2)\n",
    "        return out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OA9aXUkleYlG"
   },
   "source": [
    "## Implémentation de la double couche d'Embedding\n",
    "\n",
    "La séquence d'entrée est convertie en *Embedding* de dimension ```embed_dim```.\n",
    "L'*Embedding* final est constitué de la somme de deux *Embedding*, le premier encodant un mot, et le second encodant la position du mot dans la séquence.\n",
    "\n",
    "La couche d'*Embedding* de Keras (```layers.Embedding```) est une sorte de table associant à un indice en entrée un vecteur de dimension ```embed_dim```. Chaque coefficient de cette table est en fait un paramètre apprenable.\n",
    "\n",
    "D'après vous combien de paramètres contiendrait une couche d'*Embedding* associant un vecteur de dimension 32 à chacun des 20000 mots les plus courants du vocabulaire extrait de la base de données que nous allons utiliser ?\n",
    "Et combien pour l'*Embedding* qui associe un vecteur de dimension 32 à chaque position d'un séquence de longueur ```maxlen``` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RPGCy1t8eYlG"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        # Définition des différentes couches qui composent le bloc Embedding\n",
    "        # Embedding de mot\n",
    "        self.token_emb = layers.Embedding(embed_dim, vocab_size)\n",
    "        # Embedding de position\n",
    "        self.pos_emb = layers.Embedding(embed_dim, maxlen)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calcul de l'embedding à partir de l'entrée x\n",
    "        # ATTENTION : UTILISER UNIQUEMENT DES FONCTIONS TF POUR CETTE PARTIE\n",
    "        # Récupération de la longueur de la séquence\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        # Création d'un vecteur [0, 1, ..., maxlen] des positions associées aux\n",
    "        # mots de la séquence\n",
    "        positions = tf.Tensor([i for i in range(maxlen + 1)])\n",
    "        # Calcul des embeddings de position\n",
    "        positions_emb = self.pos_emb(positions, maxlen)\n",
    "        # Calcul des embeddings de mot\n",
    "        words_emb = self.pos_emb(x, vocab_size)\n",
    "        return positions_emb + words_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ijFa-H_eYlG"
   },
   "source": [
    "## Préparation de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3EciJaw4eYlG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 séquences d'apprentissage\n",
      "25000 séquences de validation\n"
     ]
    }
   ],
   "source": [
    "# Taille du vocabulaire considéré (on ne conserve que les 20000 mots les plus courants)\n",
    "vocab_size = 20000\n",
    "# Taille maximale de la séquence considérée (on ne conserve que les 200 premiers mots de chaque commentaire)\n",
    "maxlen = 200\n",
    "\n",
    "# Chargement des données de la base IMDB\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "print(len(x_train), \"séquences d'apprentissage\")\n",
    "print(len(x_val), \"séquences de validation\")\n",
    "\n",
    "# Padding des séquences : ajout de \"0\" pour compléter les séquences trop courtes\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CkmQNEReYlH"
   },
   "source": [
    "## Création du modèle\n",
    "\n",
    "Pour assembler le modèle final, il faut, partant d'une séquence de longueur ```maxlen```, calculer les Embedding puis les fournir en entrée d'une série de blocs Transformer. Pour ce TP, **commencez par ne mettre qu'un seul bloc Transformer**. Vous pourrez en ajouter plus tard si vous le souhaitez.\n",
    "\n",
    "Pour construire la tête de projection du réseau, vous pouvez moyenner les activations en sortie du bloc Transformer par élément de la séquence grâce à un *Global Average Pooling* (1D !), à relier à une couche dense (par exemple comportant 20 neurones) et enfin à la couche de sortie du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiRdlO_SeYlH"
   },
   "outputs": [],
   "source": [
    "embed_dim = 32  # Dimension de l'embedding pour chaque mot\n",
    "num_heads = 2  # Nombre de têtes d'attention\n",
    "\n",
    "# A COMPLETER\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "emb = TokenAndPositionEmbedding(maxlen, vocab_len, embed_dim)(inputs)\n",
    "x = TransformerBlock(embed_dim,num_heads)(emb)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(20, activation='relu')(x)\n",
    "outputs = layers.Dense(2, activation= 'relu')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TZq2_8kopTL"
   },
   "source": [
    "Enfin vous pouvez lancer l'apprentissage, avec par exemple l'optimiseur Adam. Inutile de lancer de trop nombreuses *epochs*, le réseau sur-apprend très vite !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfeEvasteYlH"
   },
   "outputs": [],
   "source": [
    "# A COMPLETER\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"...\", metrics=[\"...\"]\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5XtVsJLo5h2"
   },
   "source": [
    "**Questions subsidiaires**:\n",
    "\n",
    "\n",
    "\n",
    "1.   Comparez les résultats à ceux d'un LSTM bi-directionnel.\n",
    "2.   **Plus dur**: GPT bénéficie d'un pré-entraînement non supervisé sur des données issues de gigantesques bases de texte. L'idée, résumée dans l'extrait de l'article copié ci-dessous, consiste à pré-entraîner le modèle à prédire le prochain mot d'une séquence fournie. De larges bases de données, comme [WikiText](https://huggingface.co/datasets/wikitext), permettent de pré-entraîner efficacement le réseau, particulièrement dans notre cas la couche d'*Embedding* qui contient la majorité des paramètres du réseau.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?id=1RWPVSAEA5frRvqHkOxw6h1MDRe-fT0sC\"> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "resZBBd2GL3x"
   },
   "source": [
    "## Quelques éléments pour aller plus loin\n",
    "\n",
    "Chargement de la base de données WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Alov-5qysW1_"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvaMXYZ1Gcxi"
   },
   "source": [
    "La difficulté est maintenant de travailler cette base de données pour produire des séquences, en réutilisant les mêmes numéros de tokens de la base IMDB utilisée précédemment..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
