{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMMppWbnG3dN"
   },
   "source": [
    "# Apprentissage Semi-Supervisé\n",
    "\n",
    "On se propose dans ce TP d'illustrer certaines techniques d'apprentissage semi-supervisé vues en cours.\n",
    "\n",
    "Dans tout ce qui suit, on considère que l'on dispose d'un ensemble de données $x_{lab}$ labellisées et d'un ensemble de donnés $x_{unlab}$ non labellisées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2onzaW7mJrgG"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BejOODdKZ70"
   },
   "source": [
    "Commencez par exécuter ces codes qui vos permettront de charger les datasets que nous allons utiliser et de les séparer en données labellisées et non labellisées, ainsi qu'en données de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2nYQ2X5JW2k"
   },
   "source": [
    "### Dataset des deux clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pkv-k9qIJyXH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250):\n",
    "  num_samples = num_lab + num_unlab + num_test\n",
    "  # Génération de 1000 données du dataset des 2 lunes\n",
    "  x, y = datasets.make_blobs(n_samples=[round(num_samples/2), round(num_samples/2)], n_features=2, center_box=(- 3, 3), random_state=1)\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=num_test/num_samples, random_state=1)\n",
    "  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=num_unlab/(num_unlab+num_lab), random_state=6)\n",
    "\n",
    "  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBwkuDKFLKdH"
   },
   "outputs": [],
   "source": [
    "x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250)\n",
    "\n",
    "print(x_train_lab.shape, x_train_unlab.shape, x_test.shape)\n",
    "print(y_train_lab.shape, y_train_unlab.shape, y_test.shape)\n",
    "\n",
    "# Affichage des données\n",
    "plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], color=(0.5,0.5,0.5), marker='.', linestyle=' ')\n",
    "plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], color=(0.5,0.5,0.5), marker='.', linestyle=' ')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n",
    "plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKR9vNgsLp_J"
   },
   "source": [
    "### Dataset des 2 lunes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AFhsTUQwIxt"
   },
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1xb_gasBJ6sEmbyvCWTnVEAsbspyDCyFL\">\n",
    "<caption><center> Figure 1: Comparaison de différents algorithmes semi-supervisés sur le dataset des 2 lunes</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCw5v2JDLwau"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_2moons_dataset(num_lab = 10, num_unlab=740, num_test=250):\n",
    "  num_samples = num_lab + num_unlab + num_test\n",
    "  # Génération de 1000 données du dataset des 2 lunes\n",
    "  x, y = datasets.make_moons(n_samples=num_samples, noise=0.1, random_state=1)\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=num_test/num_samples, random_state=1)\n",
    "  x_train_lab, x_train_unlab, y_train_lab, y_train_unlab = train_test_split(x_train, y_train, test_size=num_unlab/(num_unlab+num_lab), random_state=6)\n",
    "\n",
    "  return x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkQ1L5I1MBkH"
   },
   "outputs": [],
   "source": [
    "x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2moons_dataset(num_lab = 10, num_unlab=740, num_test=250)\n",
    "\n",
    "print(x_train_lab.shape, x_train_unlab.shape, x_test.shape)\n",
    "print(y_train_lab.shape, y_train_unlab.shape, y_test.shape)\n",
    "\n",
    "# Affichage des données\n",
    "plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], 'b.')\n",
    "plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n",
    "plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIGZe-yAQq-A"
   },
   "source": [
    "## Modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaPezVmtK5tC"
   },
   "source": [
    "Nous allons dès maintenant préparer les modèles que nous utiliserons dans la suite.\n",
    "\n",
    "**Travail à faire** Complétez les modèles ci-dessous :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfw8EKUuUpt6"
   },
   "source": [
    "Pour le dataset des 2 clusters, un simple perceptron monocouche suffira :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BryV3CDKQytA"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Model, Input\n",
    "# A COMPLETER\n",
    "# Ici, écrire un simple perceptron monocouche\n",
    "def create_model_2clusters():\n",
    "\n",
    "  inputs = Input(shape=(2,))\n",
    "  # A COMPLETER\n",
    "  outputs = ...\n",
    "\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrE8ZQCpUuxg"
   },
   "source": [
    "Pour le dataset des 2 lunes, implémentez un perceptron multi-couches à une couche cachée, par exemple de 20 neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1jcG_4pyGlx"
   },
   "outputs": [],
   "source": [
    "# A COMPLETER\n",
    "# Ici, écrire un perceptron multi-couches à une seule couche cachée comprenant 20 neurones\n",
    "def create_model_2moons():\n",
    "\n",
    "  inputs = keras.Input(shape=(...))\n",
    "\n",
    "  # A COMPLETER\n",
    "\n",
    "  outputs = ...\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMaTgZJcQbIh"
   },
   "source": [
    "## Apprentissage supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGTfv5YfMAXY"
   },
   "source": [
    "Commencez par bien lire le code ci-dessous, qui vous permet de mettre en place un apprentissage supervisé en détaillant la boucle d'apprentissage (de l'algorithme de descente de gradient stochastique). Cela nous permettra d'avoir plus de contrôle dans la suite pour implémenter les algorithmes semi-supervisés. Cela vous fournira également une base contre laquelle comparer les algorithmes semi-supervisés.\n",
    "\n",
    "En quelques mots, l'algorithme est organisé autour d'une double boucle : une sur les *epochs*, et la 2nde sur les *mini-batches*.\n",
    "\n",
    "Pour chaque nouveau batch de données, on réalise la succession d'étapes suivantes dans un bloc **GradientTape** qui permet le calcul automatique des gradients :     \n",
    "\n",
    "\n",
    "1.   Prédiction de la sortie du modèle sur les données du batch\n",
    "2.   Calcul de la fonction de perte entre sortie du réseau et labels réels associés aux élements du batch\n",
    "3.   Calcul des gradients de la perte par rapport aux paramètres du réseau (par différentiation automatique)\n",
    "4.   Mise à jour des paramètres grâce aux gradients calculés.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbmhai8PVXVd"
   },
   "source": [
    "### Dataset des 2 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP5XgJRQQm5_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import math\n",
    "\n",
    "# Données et modèle du problème des 2 clusters\n",
    "x_train_lab, y_train_lab, x_train_unlab, y_train_unlab, x_test, y_test = generate_2clusters_dataset(num_lab = 10, num_unlab=740, num_test=250)\n",
    "model = create_model_2clusters()\n",
    "\n",
    "# Hyperparamètres de l'apprentissage\n",
    "epochs = 150\n",
    "batch_size = 16\n",
    "if batch_size < x_train_lab.shape[0]:\n",
    "  steps_per_epoch = math.floor(x_train_lab.shape[0]/batch_size)\n",
    "else:\n",
    "  steps_per_epoch = 1\n",
    "  batch_size = x_train_lab.shape[0]\n",
    "\n",
    "# Instanciation d'un optimiseur et d'une fonction de coût.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Préparation des métriques pour le suivi de la performance du modèle.\n",
    "train_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "test_acc_metric = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "# Indices de l'ensemble labellisé\n",
    "indices = np.arange(x_train_lab.shape[0])\n",
    "\n",
    "# Boucle sur les epochs\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  # A chaque nouvelle epoch, on randomise les indices de l'ensemble labellisé\n",
    "  np.random.shuffle(indices)\n",
    "\n",
    "  # Et on recommence à cumuler la loss\n",
    "  cum_loss_value = 0\n",
    "\n",
    "  for step in range(steps_per_epoch):\n",
    "\n",
    "    # Sélection des données du prochain batch\n",
    "    x_batch = x_train_lab[indices[step*batch_size: (step+1)*batch_size]]\n",
    "    print(np.shape(x_batch))\n",
    "    y_batch = y_train_lab[indices[step*batch_size: (step+1)*batch_size]]\n",
    "\n",
    "    # Etape nécessaire pour comparer y_batch à la sortie du réseau\n",
    "    y_batch = np.expand_dims(y_batch, 1)\n",
    "\n",
    "    # Les opérations effectuées par le modèle dans ce bloc sont suivies et permettront\n",
    "    # la différentiation automatique.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "      # Application du réseau aux données d'entrée\n",
    "      y_pred = model(x_batch, training=True)  # Logits for this minibatch\n",
    "\n",
    "      # Calcul de la fonction de perte sur ce batch\n",
    "      loss_value = loss_fn(y_batch, y_pred)\n",
    "\n",
    "      # Calcul des gradients par différentiation automatique\n",
    "      grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "      # Réalisation d'une itération de la descente de gradient (mise à jour des paramètres du réseau)\n",
    "      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "      # Mise à jour de la métrique\n",
    "      train_acc_metric.update_state(y_batch, y_pred)\n",
    "\n",
    "      cum_loss_value = cum_loss_value + loss_value\n",
    "\n",
    "  # Calcul de la précision à la fin de l'epoch\n",
    "  train_acc = train_acc_metric.result()\n",
    "\n",
    "  # Calcul de la précision sur l'ensemble de test à la fin de l'epoch\n",
    "  test_logits = model(x_test, training=False)\n",
    "  test_acc_metric.update_state(np.expand_dims(y_test, 1), test_logits)\n",
    "  test_acc = test_acc_metric.result()\n",
    "\n",
    "  print(\"Epoch %4d : Loss : %.4f, Acc : %.4f, Test Acc : %.4f\" % (epoch, float(cum_loss_value/steps_per_epoch), float(train_acc), float(test_acc)))\n",
    "\n",
    "  # Remise à zéro des métriques pour la prochaine epoch\n",
    "  train_acc_metric.reset_states()\n",
    "  test_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcnTF5WWVacl"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Affichage des données\n",
    "plt.plot(x_train_unlab[y_train_unlab==0,0], x_train_unlab[y_train_unlab==0,1], 'b.')\n",
    "plt.plot(x_train_unlab[y_train_unlab==1,0], x_train_unlab[y_train_unlab==1,1], 'r.')\n",
    "\n",
    "plt.plot(x_test[y_test==0,0], x_test[y_test==0,1], 'b+')\n",
    "plt.plot(x_test[y_test==1,0], x_test[y_test==1,1], 'r+')\n",
    "\n",
    "plt.plot(x_train_lab[y_train_lab==0,0], x_train_lab[y_train_lab==0,1], 'b.', markersize=30)\n",
    "plt.plot(x_train_lab[y_train_lab==1,0], x_train_lab[y_train_lab==1,1], 'r.', markersize=30)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#Affichage de la frontière de décision\n",
    "plot_decision_regions(x_train_unlab, y_train_unlab, clf=model, legend=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPiuBS36V8EG"
   },
   "source": [
    "# Minimisation de l'entropie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlBFMsFLMtEp"
   },
   "source": [
    "Nous allons dans cette partie implémenter l'algorithme de minimisation de l'entropie de prédiction du modèle sur les données non labellisées, conjointement à la minimisation de l'entropie croisée sur les données labellisées.\n",
    "\n",
    "Pour commencer, implémentez la fonction de coût qui calcule l'entropie $H$ des prédictions du réseau $\\hat{y}$ :\n",
    "$$ H(\\hat{y}) = -  \\hat{y} log(\\hat{y}) - (1 - \\hat{y}) log(1 - \\hat{y})  $$\n",
    "\n",
    "En réalité, nous travaillons sur des batches de $m$ échantillons ; pour obtenir l'entropie d'un batch $B$, il suffit de moyenner l'entropie de tous les échantillons du batch :\n",
    "\n",
    "$$ H(B) = \\frac{1}{m} \\sum_{i=1}^{m} H(y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gEt2x_sXFin"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Calcul de l'entropie de y_pred\n",
    "# A COMPLETER\n",
    "def binary_entropy_loss(y_pred):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_AHlNlip2Wu"
   },
   "source": [
    "**ATTENTION: il faut gérer manuellement les cas où $\\hat{y}$ vaudrait 0 ou 1, ce qui donnerait une valeur *NaN* à la fonction de coût.** Pour cela, vous pouvez utiliser la fonction [clip_by_value](https://www.tensorflow.org/api_docs/python/tf/clip_by_value) qui va vous permettre de circonscrire les valeurs de $\\hat{y}$ à $[\\epsilon , 1 - \\epsilon ] $ (vous pouvez par exemple utiliser la valeur $\\epsilon = 10^{-7}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPHbvm-6q5a-"
   },
   "source": [
    "**ATTENTION 2**: cette fonction de coût doit être dérivable et doit donc être implémentée uniquement en utilisant des fonctions de *Tensorflow* (et surtout pas *Numpy*) : *tf.reduce_mean()*, *tf.log()*, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L1Li1YtNN87"
   },
   "source": [
    "**Travail à faire** : Reprenez maintenant la boucle d'apprentissage supervisé et introduisez la minimisation d'entropie pour régulariser l'apprentissage.\n",
    "\n",
    "La difficulté principale va être l'introduction des données non labellisées dans la boucle. Ainsi, un batch devra maintenant être composé de données labellisées et non labellisées. Je vous suggère de conserver le même nombre de données labellisées par batch que précédemment (i.e. 16) et de prendre un plus grand nombre de données non labellisées, par exemple 90.\n",
    "\n",
    "N'oubliez pas également d'introduire un hyperparamètre λ pour contrôler l'équilibre entre perte supervisée et non supervisée. Utilisez un λ constant dans un premier temps, et testez ensuite des variantes qui consisteraient à augmenter progressivement sa valeur au fil des epochs.\n",
    "\n",
    "La fonction objectif à minimiser aura donc la forme :    \n",
    "$$  J = \\sum_{(x,y) \\in \\mathcal{L}} CE(y, \\hat{y}) + \\lambda \\sum_{x \\in \\mathcal{U}} H(\\hat{y})\t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y25fa_IIZeuH"
   },
   "source": [
    "Une fois cette étape réalisée, vous pouvez tester l'algorithme sur le dataset des 2 lunes ; comme annoncé en cours, vous devriez avoir beaucoup de mal à faire fonctionner l'algorithme sur ces données.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
