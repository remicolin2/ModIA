{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e42f156-3e40-45bb-8b0a-4cad8daa7b03",
   "metadata": {
    "id": "XMMppWbnG3dN"
   },
   "source": [
    "<font size=\"+3\">Estimation de posture dans une image</font>\n",
    "\n",
    "---\n",
    "\n",
    "Pour ce TP, nous allons traiter le problème de la détection du \"squelette\" d'un humain dans une image, tel qu'illustré dans la figure ci-dessous.\n",
    "\n",
    "![Texte alternatif…](https://drive.google.com/uc?id=1HpyLwzwkFdyQ6APoGZQJL7f837JCHNkh)\n",
    "\n",
    "Nous allons pour ce faire utiliser le [Leeds Sport Pose Dataset](https://sam.johnson.io/research/lspet.html) qui introduit 10000 images présentant des sportifs dans diverses situations, augmentées d'une annotation manuelle du squelette.\n",
    "\n",
    "À chaque image est associée une matrice de taille 3x14, correspondant aux coordonnées dans l'image des 14 joints du squelette de la personne décrite dans l'image. La 3e dimension désigne la visibilité du joint (1 s'il est visible, 0 s'il est occulté)\n",
    "\n",
    "Ces joints sont, dans l'ordre :\n",
    "*   Cheville droite\n",
    "*   Genou droit\n",
    "*   Hanche droite\n",
    "*   Hanche gauche\n",
    "*   Genou gauche\n",
    "*   Cheville gauche\n",
    "*   Poignet droit\n",
    "*   Coude droit\n",
    "*   Épaule droite\n",
    "*   Épaule gauche\n",
    "*   Coude gauche\n",
    "*   Poignet gauche\n",
    "*   Cou\n",
    "*   Sommet du crâne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89baee-2871-4867-afdb-f9dc7ebca493",
   "metadata": {},
   "source": [
    "# Méthodologie \n",
    "\n",
    "Pour résoudre ce problème, nous allons suivre une méthodologie similaire à celle présentée dans le cours d'introduction, et rappelée sur la figure suivante : \n",
    "\n",
    "![Méthodologie de développement d'un algorithme d'apprentissage profond](https://drive.google.com/uc?id=195pkcjca4r_g86KDt2LCe0QdQsMC6iba)\n",
    "\n",
    "Ainsi nous allons commencer par une modélisation simple du problème, construire un modèle et l'améliorer pas à pas et évaluer sa performance.\n",
    "Dans un second temps, nous modifierons la modélisation du problème, et donc l'architecture utilisée, afin d'améliorer les résultats.\n",
    "\n",
    "Pour chacune de ces deux étapes, je vous suggère de suivre la démarche suivante : \n",
    "\n",
    "- Simplifier le problème en traitant 10 imagettes (par exemple de dimension $64 \\times 64$) et construire un réseau qui surapprend parfaitement (qui diminue la perte jusqu'à quasiment 0)\n",
    "- Ajouter des images (~1000) et recalibrer le réseau pour à nouveau, obtenir un sur-apprentissage\n",
    "- Commencer à corriger le sur-apprentissage en ajoutant de la régularisation\n",
    "- Et enfin, utiliser l'ensemble de la base de données pour diminuer le sur-apprentissage au maximum\n",
    "\n",
    "---\n",
    "\n",
    "Commencez par télécharger la base de données sur Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fba7b-1e59-413a-b50a-6602d15af584",
   "metadata": {
    "id": "3IVjmLKWRDag"
   },
   "outputs": [],
   "source": [
    "!git clone https://plmlab.math.cnrs.fr/chevallier-teaching/datasets/leeds-sport-pose.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0489f6e-5aea-4571-a72a-5cd73c3e0ab6",
   "metadata": {
    "id": "qjqZNAX2CVi1"
   },
   "source": [
    "# **Approche n°1** : Régression de la position des joints\n",
    "\n",
    "Dans un premier temps, et comme vu en cours, nous allons nous inspirer de l'algorithme DeepPose (**[[Toshev et al.]](https://arxiv.org/abs/1312.4659)** _DeepPose : Human Pose Estimation via Deep Neural Networks_) et formuler le problème comme une régression de la position $(x,y)$ des joints dans l'espace de l'image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b40520-a19e-4056-87b8-386af0a7ca8e",
   "metadata": {},
   "source": [
    "## Fonctions utiles\n",
    "\n",
    "Le bloc suivant contient une fonction qui permet de charger les images de la base de données dans les variables $x$ et $y$. Par défaut les images sont redimensionnées en taille 128$\\times$128 et la base de données contient 1000 images. Pour commencer et vous permettre de travailler plus efficacement, **je vous suggère très fortement de diminuer la dimension des images** (par exemple 64$\\times$64) **et de ne travailler que sur un ensemble réduit d'images** (par exemple, 10). \n",
    "\n",
    "\n",
    "N'oubliez pas également de diviser les données en images de test et/ou de validation pour obtenir des informations sur le sur-apprentissage éventuel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05833256-35d4-4ee5-8476-803118f1f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676aea7-a55e-46c5-abeb-ee93b11ae49f",
   "metadata": {
    "id": "quOHEF__pf36"
   },
   "outputs": [],
   "source": [
    "# Cette fonction permettra plus tard de charger plus ou moins d'images (en modifiant le paramètre num_images)\n",
    "# et de modifier la dimension d'entrée\n",
    "\n",
    "def load_data(image_size=128, num_images=1000):\n",
    "\n",
    "    path = \"./leeds-sport-pose/images/\"\n",
    "    dirs = sorted(os.listdir(path))\n",
    "\n",
    "    x = np.zeros((min(num_images,len(dirs)), image_size,image_size,3))\n",
    "    y = np.zeros((min(num_images,len(dirs)), 3, 14))\n",
    "\n",
    "    #Chargement des joints    \n",
    "    mat_contents = loadmat('./leeds-sport-pose/joints.mat')\n",
    "    joints = mat_contents['joints']\n",
    "\n",
    "    # Chargement des images, qui sont rangées dans leeds-sport-pose/images\n",
    "    for i in range(min(num_images,len(dirs))):\n",
    "        item = dirs[i]\n",
    "        if os.path.isfile(path+item):\n",
    "            img = Image.open(path+item)\n",
    "            # Redimensionnement et sauvegarde des joints\n",
    "            y[i, 0] = joints[:,0,i]*image_size/img.size[0]\n",
    "            y[i, 1] = joints[:,1,i]*image_size/img.size[1]\n",
    "            y[i, 2] = joints[:,2,i]\n",
    "            # Redimensionnement et sauvegarde des images        \n",
    "            img = img.resize((image_size,image_size))\n",
    "            x[i] = np.asarray(img)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Chargement de seulement 10 images, de taille 64x64\n",
    "x, y = load_data(image_size=64, num_images=10)           \n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4754aa6-80c9-43a3-a015-a1d6eb18c9be",
   "metadata": {
    "id": "zRc0B4oxe6h_"
   },
   "outputs": [],
   "source": [
    "labels= {0: 'Cheville droite',\n",
    "         1: 'Genou droit',\n",
    "         2: 'Hanche droite',\n",
    "         3: 'Hanche gauche',\n",
    "         4: 'Genou gauche',\n",
    "         5: 'Cheville gauche',\n",
    "         6: 'Poignet droit',\n",
    "         7: 'Coude droit',\n",
    "         8: 'Épaule droite',\n",
    "         9: 'Épaule gauche',\n",
    "         10: 'Coude gauche',\n",
    "         11: 'Poignet gauche',\n",
    "         12: 'Cou',\n",
    "         13: 'Sommet du crâne'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e5123-4564-4284-bef0-c3ee0b620ad2",
   "metadata": {
    "id": "meezS1y4G8QO"
   },
   "source": [
    "La fonction suivante vous permet de visualiser les données. Vous vous rendrez compte que certaines données sont manquantes ! En effet quand des joints sont occultés dans les images, des valeurs de position aberrantes (négatives) sont indiquées. Dans ce cas, nous n'afficherons pas les articulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd270b8-2dff-4d1b-b905-b00dfe9ff981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d398ea-2998-423a-812f-811627243291",
   "metadata": {
    "id": "JvcqdQIZdCYk"
   },
   "outputs": [],
   "source": [
    "# Fonction d'affichage d'une image et de son label associé\n",
    "\n",
    "def print_data(x,y,i):\n",
    "\n",
    "    if y.shape[1] < 3:\n",
    "        y_new = np.ones((y.shape[0], 3, y.shape[2]))\n",
    "        y_new[:,0:2,:] = y\n",
    "        y = y_new\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(x[i]/255)\n",
    "    for j in range(0,14):\n",
    "        if y[i, 2, j] == 1:\n",
    "            plt.scatter(y[i,0,j],y[i,1,j],label=labels.get(j))\n",
    "\n",
    "    # Jambe droite      \n",
    "    if (y[i, 2, 0] + y[i, 2, 1] == 2):\n",
    "        plt.plot(y[i,0,0:2],y[i,1,0:2],'b')\n",
    "    # Cuisse droite      \n",
    "    if (y[i, 2, 1] + y[i, 2, 2] == 2):\n",
    "        plt.plot(y[i,0,1:3],y[i,1,1:3],'b')\n",
    "    # Bassin     \n",
    "    if (y[i, 2, 2] + y[i, 2, 3] == 2):\n",
    "        plt.plot(y[i,0,2:4],y[i,1,2:4],'b')\n",
    "    # Cuisse gauche      \n",
    "    if (y[i, 2, 3] + y[i, 2, 4] == 2):\n",
    "        plt.plot(y[i,0,3:5],y[i,1,3:5],'b')\n",
    "    # Jambe gauche      \n",
    "    if (y[i, 2, 4] + y[i, 2, 5] == 2):\n",
    "        plt.plot(y[i,0,4:6],y[i,1,4:6],'b')\n",
    "    # Avant-bras droit      \n",
    "    if (y[i, 2, 6] + y[i, 2, 7] == 2):\n",
    "        plt.plot(y[i,0,6:8],y[i,1,6:8],'b')\n",
    "    # Bras droit      \n",
    "    if (y[i, 2, 7] + y[i, 2, 8] == 2):\n",
    "        plt.plot(y[i,0,7:9],y[i,1,7:9],'b')\n",
    "    # Bras gauche     \n",
    "    if (y[i, 2, 9] + y[i, 2, 10] == 2):\n",
    "        plt.plot(y[i,0,9:11],y[i,1,9:11],'b')\n",
    "    # Avant-bras gauche      \n",
    "    if (y[i, 2, 10] + y[i, 2, 11] == 2):\n",
    "        plt.plot(y[i,0,10:12],y[i,1,10:12],'b') \n",
    "    # Buste droit\n",
    "    x1=[y[i,0,2],y[i,0,12]]\n",
    "    y1=[y[i,1,2],y[i,1,12]]\n",
    "    if (y[i, 2, 2] + y[i, 2, 12] == 2):\n",
    "        plt.plot(x1, y1,'b')\n",
    "    # Buste gauche\n",
    "    x1=[y[i,0,3],y[i,0,12]]\n",
    "    y1=[y[i,1,3],y[i,1,12]]\n",
    "    if (y[i, 2, 3] + y[i, 2, 12] == 2):\n",
    "        plt.plot(x1, y1,'b')\n",
    "    # Omoplate droite\n",
    "    x1=[y[i,0,8],y[i,0,12]]\n",
    "    y1=[y[i,1,8],y[i,1,12]]\n",
    "    if (y[i, 2, 8] + y[i, 2, 12] == 2):\n",
    "        plt.plot(x1, y1,'b')\n",
    "    # Omoplate gauche\n",
    "    x1=[y[i,0,9],y[i,0,12]]\n",
    "    y1=[y[i,1,9],y[i,1,12]]\n",
    "    if (y[i, 2, 9] + y[i, 2, 12] == 2):\n",
    "        plt.plot(x1, y1,'b')\n",
    "    # Tete     \n",
    "    if (y[i, 2, 12] + y[i, 2, 13] == 2):\n",
    "        plt.plot(y[i,0,12:14],y[i,1,12:14],'b')\n",
    "\n",
    "    plt.axis([0, x.shape[1], x.shape[2], 0])\n",
    "    plt.show()\n",
    "    #plt.legend()\n",
    "\n",
    "# Affichage aléatoire d'une image\n",
    "print_data(x,y,np.random.randint(x.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a38b9-e6db-4bf7-8ef8-6061b4abcc69",
   "metadata": {},
   "source": [
    "Si nous formulons ce problème comme une régression, nous allons utiliser pour évaluer nos réseaux de neurones l'erreur quadratique moyenne (fonction *MSE*). Cette fonction sera parfaite comme fonction de perte, mais elle ne permet pas d'appréhender les résultats de manière satisfaisante.\n",
    "\n",
    "Une métrique commune en estimation de posture est le **PCK0.5**, pour *Percentage of Correct Keypoints*. *0.5* correspond à un seuil en-deça duquel on considère qu'un joint est correctement estimé. Cette question du seuil est particulièrement sensible car il faut utiliser une valeur qui soit valable pour n'importe quelle image. La personne considérée peut apparaître plus ou moins largement sur l'image, de face ou de profil, ce qui fait qu'une erreur de prédiction sur un joint peut avoir une importance très grande ou très faible selon les cas.\n",
    "\n",
    "Pour résoudre cette ambiguïté, on considère dans la métrique du **PCK0.5** que la référence est la taille de la tête, définie par la distance entre le joint du cou et le joint de la tête sur la vérité terrain. Un joint prédit par le réseau sera considéré correct s'il est situé à une distance inférieure à la moitié (*0.5*) de la taille de la tête par rapport au joint réel. (**[[Andriluka et al.]](https://openaccess.thecvf.com/content_cvpr_2014/html/Andriluka_2D_Human_Pose_2014_CVPR_paper.html)** _2D Human Pose Estimation: New Benchmark and State of the Art Analysis_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7bead-5221-408e-9dc5-0e3f485b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.matlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ff9a2-6843-4a80-a498-2af70ac860ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du \"Percentage of Correct Keypoint\" avec seuil alpha :\n",
    "# On compte corrects les joints pour lesquels la distance entre valeurs réelle et prédite \n",
    "# est inférieure à alpha fois la dimension de la tête (c'est un peu arbitraire...)\n",
    "# On ne comptera pas les joints invisibles.\n",
    "# y_true est de dimension Nx3x14 et y_pred Nx2x14 (le réseau ne prédit pas la visibilité)\n",
    "\n",
    "def compute_PCK_alpha(y_true, y_pred, alpha=0.5):\n",
    "    # Calcul des seuils ; la taille de la tête est la distance entre joints 12 et 13\n",
    "    head_sizes = np.sqrt( np.square(y_true[:,0,13]-y_true[:,0,12]) + np.square(y_true[:,1,13]-y_true[:,1,12]) )\n",
    "    thresholds = alpha * head_sizes\n",
    "    thresholds = np.matlib.repmat(np.expand_dims(thresholds, 1), 1, 14)\n",
    "\n",
    "    # Calcul des distances inter-joints\n",
    "    joints_distances = np.sqrt( np.square(y_true[:,0,:]-y_pred[:,0,:]) + np.square(y_true[:,1,:]-y_pred[:,1,:]) )\n",
    "\n",
    "    # Visibilité des joints de la vérité terrain\n",
    "    visibility = y_true[:,2,:]\n",
    "    \n",
    "    total_joints = np.count_nonzero(visibility==1)\n",
    "    correctly_predicted_joints = np.count_nonzero( np.logical_and(joints_distances<thresholds, visibility == 1) )\n",
    "    \n",
    "    return correctly_predicted_joints/total_joints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c400ac-01a3-4b1c-9e9c-65c6b77dae31",
   "metadata": {},
   "source": [
    "Comme dit précédemment, on va utiliser l'erreur quadratique moyenne (*[MSE](https://en.wikipedia.org/wiki/Mean_squared_error)*) comme fonction de coût pour entraîner notre réseau de neurones, et on peut également utiliser l'erreur absolue moyenne (*[MAE](https://en.wikipedia.org/wiki/Mean_absolute_error)*) pour obtenir une estimation plus fine des performances de notre réseau pendant l'entraînement (on obtient une erreur moyenne en pixels, ce qui est plus simple à interpréter).\n",
    "\n",
    "Il y a cependant une subtilité importante évoquée un peu plus haut : certains joints sont invisibles, et ont des coordonnées négatives (pour, il faut l'avouer, une raison un peu inexplicable). Il est important de ne pas affecter l'apprentissage en faisant prédire ces valeurs négatives, insensées, au réseau. \n",
    "\n",
    "On doit donc implanter nous-même notre propre fonction de coût, qui ne va pas prendre en compte les joints invisibles. Pour cela, il faut savoir que la vérité-terrain contient en fait 3 valeurs pour chaque joint : les 2 premières sont ses coordonnées sur l'image, la 3e représente la visibilité du joint (1 s'il est visible, 0 sinon).\n",
    "\n",
    "La fonction *custom_mse*, définie juste en-dessous, réalise cette opération. Prenez le temps de comprendre ce qu'il s'y passe. \n",
    "\n",
    "**Remarque importante** : Ce code fait appel à des fonctions particulières du Backend de `Keras`, dont vous trouverez les détails sur [cette page](https://keras.rstudio.com/articles/backend.html). Ces fonctions doivent traiter des tenseurs, de type _tensor_ (et pas des tableaux _numpy_), car elles seront appelées pendant l'entraînement sur des variables internes à `Tensorflow`. Les fonctions utilisables sont également limitées car il faut pouvoir dériver la fonction *custom_mse* pour la rétropropagation des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a116d8d-f0ae-45fa-b6c8-32d66950c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cda88-5f15-41a3-b3c0-08cd959197fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true : vérité terrain de dimension B x 3 x 14\n",
    "# y_pred : une prédiction de dimension B x 2 x 14 (on ne prédit pas la visibilité)\n",
    "# B est le nombre d'images considérées (par exemple, pourra être la taille d'un mini-batch)\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    # Changement de dimension : Bx3x14 -> Bx14x3\n",
    "    y_true = K.permute_dimensions(y_true, (0, 2, 1))\n",
    "    # Changement de dimension : Bx14x3 -> (B*14)x3\n",
    "    y_true = K.reshape(y_true, shape=(-1, 3))\n",
    "  \n",
    "    # Changement de dimension : Bx2x14 -> Bx14x2\n",
    "    y_pred = K.permute_dimensions(y_pred, (0, 2, 1))\n",
    "    # Changement de dimension : Bx14x2 -> (B*14)x2\n",
    "    y_pred = K.reshape(y_pred, shape=(-1, 2))\n",
    "    \n",
    "    # Détermination de l'indices des joints visibles\n",
    "    visible = K.greater_equal(y_true[:, 2], 1)  \n",
    "    indices = K.arange(0, K.shape(y_true)[0])\n",
    "    indices_visible = indices[visible]\n",
    "    \n",
    "    # Sélection des vérité-terrains et prédictions des joints visibles\n",
    "    y_true_visible = K.gather(y_true[:,0:2], indices_visible)\n",
    "    y_pred_visible = K.gather(y_pred, indices_visible)\n",
    "    \n",
    "    # Calcul de la MSE\n",
    "    return K.mean( K.square(y_pred_visible[:,0]-y_true_visible[:,0]) + K.square(y_pred_visible[:,1]-y_true_visible[:,1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac684-b6f0-43fe-b4de-194d1096fe93",
   "metadata": {},
   "source": [
    "Si vous avez bien compris le code de *custom_mse*, vous devriez pouvoir sans trop de problèmes écrire le code pour la fonction *custom_mae* ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b99c5-c12b-4210-b908-c06280b9a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true : vérité terrain de dimension B x 3 x 14\n",
    "# y_pred : une prédiction de dimension B x 2 x 14 (on ne prédit pas la visibilité)\n",
    "# B est le nombre d'images considérées (par exemple, pourra être la taille d'un mini-batch)\n",
    "\n",
    "def custom_mae(y_true, y_pred):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e40fe5-6881-4268-84e7-bf48364072b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/custom_mae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3717c359-777a-46ee-9561-c8c983bec82c",
   "metadata": {},
   "source": [
    "Comme d'habitude, on peut monitorer l'entraînement grâce à la fonction suivante (adaptée à nos fonctions *custom_mse* et *custom_mae* définies juste avant) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c9b7a-b448-4119-a147-4f57f9b48fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_analysis(history):\n",
    "    mae = history.history['custom_mae']\n",
    "    val_mae = history.history['val_custom_mae']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(len(loss))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, mae, 'b', linestyle=\"--\",label='Training MAE')\n",
    "    plt.plot(epochs, val_mae, 'g', label='Validation MAE')\n",
    "    plt.title('Training and validation MAE')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, loss, 'b', linestyle=\"--\",label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'g', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f90a6-607f-475a-a2e4-9aa88affa5d8",
   "metadata": {},
   "source": [
    "## À vous de jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98103d-cd80-4692-a728-3c3960864f0b",
   "metadata": {},
   "source": [
    "Pour tenter de résoudre le problème, vous pouvez suivre les étapes suivantes : \n",
    "\n",
    "- Simplifier le problème en traitant 10 imagettes (par exemple de dimension $64 \\times 64$) et construire un réseau qui surapprend parfaitement (qui diminue la perte jusqu'à quasiment 0 $\\leadsto$ `simple_CNN.py`)\n",
    "- Ajouter des images (~1000) et éventuellement recalibrer votre réseau pour à nouveau, obtenir un sur-apprentissage\n",
    "- Commencer à corriger le sur-apprentissage en ajoutant de la régularisation (notamment sur les couches denses)\n",
    "- Et enfin, utiliser l'ensemble de la base de données pour diminuer le sur-apprentissage au maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f0214-de9b-415a-bc83-8d2cc04442bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338502d4-e811-4efc-91a7-9b49caaa4f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c588c2e-df99-4079-a1e6-6b509759aed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ce53f-fe2e-4805-9c3f-ab9b91734a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72132512-a460-43b8-b3c0-acbbd66c5612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d8cc240-91a4-46c3-b57a-fe1cef6d5773",
   "metadata": {
    "id": "JVmEnDdyB-_E"
   },
   "source": [
    "---\n",
    "\n",
    "## Petit retour sur la 1ère partie (avant de passer à la suite !) \n",
    "\n",
    "Si vous n'êtes pas allés jusqu'au bout de la partie 1, sachez que même avec un réseau bien construit, de capacité suffisante, en utilisant les 10000 images de la base d'apprentissage, de la régularisation et de l'augmentation de données (non demandé dans le TP), vous ne seriez pas arrivé à limiter le sur-apprentissage suffisamment pour obtenir des résultats satisfaisants sur l'ensemble de test. On plafonne à un pourcentage de joints correctement prédits (PCK&#64;0.5) aux alentours de 20\\%.\n",
    "\n",
    "Cela est principalement dû à la formulation du problème, plus difficile à résoudre, et aux architectures mises en place. En effet, les couches de sous-échantillonnage (*pooling*) successives entraînent une perte de précision irrémédiable qui ne peut pas être compensée par les couches denses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170741f2-a325-44bb-99c6-24ecab04dc31",
   "metadata": {
    "id": "u5rCr8gKCEnp"
   },
   "source": [
    "# **Approche n°2** : Prédiction de cartes de chaleur\n",
    "\n",
    "Dans cette partie, nous allons utiliser une autre formulation du problème, présentée pendant le cours. Plutôt que de prédire directement la position pixellique des joints, nous allons prédire des cartes de probabilité de la position des joints, comme illustré ci-dessous. Pour ce faire, il nous faudra tester des architectures de réseau de neurones différentes, s'inspirant de celles utilisées en segmentation d'image.\n",
    "\n",
    "![Texte alternatif…](https://drive.google.com/uc?id=1B8BCwQ0Szg_T_H05mnfpXFemUT5xhfxX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f49f0-9e5f-4193-85a7-0db4ccda5f1e",
   "metadata": {
    "id": "4Pqj24fMz78h"
   },
   "source": [
    "## Fonctions utiles\n",
    "\n",
    "La fonction suivante permet de créer une carte de chaleur de la dimension voulue, avec une gaussienne centrée en un point donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc37e5-a892-4965-ae6f-8ae0a4dec19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a7739e-2544-4a92-b372-2ad6a41d1cdf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "0u_t30vIEv1P",
    "outputId": "9421ae03-5b45-4e2a-f5be-91006fc2685d"
   },
   "outputs": [],
   "source": [
    "def heat_point(ind_x, ind_y, heatmap_size):\n",
    "    heat_point=np.zeros((heatmap_size,heatmap_size))\n",
    "    heat_point[ind_x][ind_y] = 1\n",
    "    \n",
    "    return gaussian_filter(heat_point, round(heatmap_size/20))\n",
    "\n",
    "\n",
    "h_m = heat_point(14, 44, 64)\n",
    "plt.imshow(h_m)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57536e-0041-4e2a-863f-373f70edf4a7",
   "metadata": {
    "id": "a1vQKbeBGI9g"
   },
   "source": [
    "On peut ensuite retrouver la position du point le plus \"chaud\", en utilisant la commande suivante : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee31e62-42df-4342-b85e-273e52a58a20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8yWcAk4lGIPa",
    "outputId": "c16937c5-c03f-4922-a288-fc0aa5e4186f"
   },
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(h_m), h_m.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77772d08-5c54-456c-86f0-f80e43420fd8",
   "metadata": {
    "id": "kiLiDhPjGieb"
   },
   "source": [
    "On peut donc définir les 2 fonctions suivantes permettant de générer les cartes de chaleur à partir des coordonnées de joints, et vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e68786-897d-4525-8d78-a60d434c825a",
   "metadata": {
    "id": "fD3PoNusGhC2"
   },
   "outputs": [],
   "source": [
    "def coord2heatmap(y, heatmap_size):\n",
    "    heatmap = np.zeros((y.shape[0], 14, heatmap_size, heatmap_size))\n",
    "    for img in range(y.shape[0]):\n",
    "        for j in range(14):\n",
    "            ind_x = int(y[img][1][j])\n",
    "            ind_y = int(y[img][0][j])\n",
    "            #print(ind_x, ind_y)\n",
    "            if ind_x >= 0 and ind_y >= 0 and ind_x < heatmap_size and ind_y < heatmap_size:\n",
    "                heatmap[img][j] = heat_point(ind_x, ind_y, heatmap_size)\n",
    "    heatmap = np.transpose(heatmap, (0, 2, 3, 1))\n",
    "    heatmap = heatmap / np.max(heatmap)\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "def heatmap2coord(heatmap):\n",
    "    y = np.ones((heatmap.shape[0], 3, 14))\n",
    "\n",
    "    heatmap = np.transpose(heatmap, (0, 3, 1, 2))\n",
    "    for img in range(y.shape[0]):\n",
    "        for j in range(14): \n",
    "            max_heat = np.unravel_index(np.argmax(heatmap[img][j]), heatmap[img][j].shape)  \n",
    "            y[img][0][j] = max_heat[1]\n",
    "            y[img][1][j] = max_heat[0]\n",
    "            if max_heat[0] == 0 and max_heat[1] == 0:\n",
    "                y[img][2][j] = 0 # Le joint est invisible\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947e5b3-d6d8-49a4-a0d6-c40921e7d803",
   "metadata": {
    "id": "TG0BlJ3LH2kP"
   },
   "source": [
    "**Todo** : Ecrivez une fonction permettant d'affichager des cartes de \n",
    "\n",
    "On affichera pour `rows` images $x$ et carte de chaleur $y$ associée : (i) l'image, (ii) tous les joints simultanément, (iii) seulement la tête, (iv) seulement le coude droit, (v) seulement le genou gauche.\n",
    "\n",
    "La cellule ci-dessous donne un exemple de résultat attendu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb00e0-ef1e-480e-aedf-823654e331f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('img/print_heatmap.png')\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a32941-d296-4e83-b156-cd8f0c40a982",
   "metadata": {
    "id": "dCfWl8ncH5JQ"
   },
   "outputs": [],
   "source": [
    "def print_heatmap(x, y, rows=3):\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ffaf7-b7a7-4310-ad1d-927e012e61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/print_heatmap.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24753a2e-b438-4903-8292-c8c89d8695e9",
   "metadata": {
    "id": "a9QKb9YLJRr4"
   },
   "source": [
    "Les lignes suivantes vous permettront de démarrer simplement : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3760e6-bcef-4677-b940-fe35366d6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea81e94-6fde-4358-bc00-4c0b49eb999d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "fwcCfcazJQL7",
    "outputId": "e4a9c11b-d7f6-4af9-f7a1-20e7dd01abfa"
   },
   "outputs": [],
   "source": [
    "# Chargement des données : on choisit une dimension d'image et de carte de chaleur de 64x64\n",
    "image_size = 64\n",
    "heatmap_size = 64\n",
    "\n",
    "# Chargement de seulement 1000 images\n",
    "x, y = load_data(image_size=image_size, num_images=1000) \n",
    "\n",
    "# Normalisation des données\n",
    "x = x/255\n",
    "\n",
    "# Chargement des données : 1000 images d'apprentissage, 100 de validation, 100 de test    \n",
    "x_train, x_gen, y_train, y_gen = train_test_split(x, y, test_size=1/10, random_state=2)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_gen, y_gen, test_size=1/2, random_state=1)\n",
    "\n",
    "# Calcul des cartes de chaleur (prend un peu de temps)\n",
    "y_hm_train = coord2heatmap(y_train, heatmap_size)\n",
    "y_hm_val = coord2heatmap(y_val, heatmap_size)\n",
    "y_hm_test = coord2heatmap(y_test, heatmap_size)\n",
    "\n",
    "# Affichage de quelques exemples\n",
    "print_heatmap(x_train, y_hm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ffa456-460b-4932-9b82-d834ee1193b0",
   "metadata": {
    "id": "QZVbag1MOphZ"
   },
   "source": [
    "Pour vous aider pour la suite, voici ci-dessous une implémentation du réseau UNet, vu en cours sur la segmentation.\n",
    "\n",
    "![Texte alternatif…](https://raw.githubusercontent.com/zhixuhao/unet/master/img/u-net-architecture.png)\n",
    "\n",
    "A vous de l'**adapter** afin qu'il soit pertinent pour le problème courant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e617809-5dec-4ba2-a81c-5af9ef6fdcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f76200-5328-4e3b-8581-a06d24843b13",
   "metadata": {
    "id": "O_qMkG8ROoky"
   },
   "outputs": [],
   "source": [
    "def create_unet(image_size=572):\n",
    "    input_layer=Input((image_size, image_size, 1))\n",
    "\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(input_layer)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input_layer, conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040ead6-b4d5-49b8-9441-cf86e5749ff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vgFsh0DRp1I3",
    "outputId": "f023b707-bfe9-4b84-b466-b52b135bbbe1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_unet(image_size=64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d2565-bdde-45cd-a6c5-5b30288221f8",
   "metadata": {
    "id": "LPamKJXZsCff"
   },
   "source": [
    "## À vous de jouer\n",
    "\n",
    "Essayez d'adapter le réseau Unet ($\\leadsto$ `modif_unet.py`) à votre problème, puis de l'entraîner sur le petit ensemble de données x_train. \n",
    "\n",
    "Plusieurs remarques (**à lire attentivement**) : \n",
    "\n",
    "\n",
    "\n",
    "*  Vous devriez observer beaucoup moins de sur-apprentissage ! Si vos prédictions sur l'ensemble de validation sont aberrantes, c'est que vous avez un bug !!\n",
    "*   Se pose la question de la formulation du problème : dans la mesure où \n",
    "les cartes de chaleur sont assimilables à des cartes de probabilité (entre 0 et 1), on peut certes choisir de conserver une formaulation du problème basée sur la régression, mais il est probablement beaucoup plus optimal de formuler le problème comme de la classification binaire.\n",
    "*   Il y a cependant toujours du sur-apprentissage. Vous pouvez donc augmenter la taille de la base de données à 10000, puis tester différentes possibilités pour diminuer ce sous-apprentissage, notamment l'augmentation de données. Les cellules suivantes vous fournissent des éléments permettant de mettre en place cette augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531621d-f9a7-4331-9753-35a2cbbf5a0d",
   "metadata": {
    "id": "JzghttyGTPvq"
   },
   "source": [
    "## Code fourni pour l'augmentation de données\n",
    "\n",
    "Vous trouverez ici quelques éléments qui vous permettront de mettre en place de l'augmentation de données. \n",
    "Attention l'augmentation rend l'apprentissage plus difficile et en fait plus lent, et vous devrez peut-être augmenter un peu la capacité de votre UNet et le nombre d'epochs pour éviter le sous-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39a9d9-40a7-40c4-8ecd-e2d47d9335f9",
   "metadata": {
    "id": "n2U4RIeNTmq_"
   },
   "source": [
    "[`Albumentation`](https://albumentations.ai/) est une librairie implémentant un grand nombre d'opérations d'augmentation de données. Dans le code suivant, deux types d'augmentation sont définies : des transformations spatiales (`ShiftScaleRotate`), et des transformations colorimétriques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e855f-a37b-430a-9318-db807703a936",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U albumentations\n",
    "!echo \"$(pip freeze | grep albumentations) is successfully installed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dabee57-9296-41c6-83f2-74c3b2931466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from albumentations import (Compose, RandomBrightnessContrast, RandomGamma, ShiftScaleRotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20956e11-cef1-4737-ae0e-5417418dd769",
   "metadata": {
    "id": "4E-o7eMQTtE5"
   },
   "outputs": [],
   "source": [
    "AUGMENTATIONS_TRAIN = Compose([\n",
    "    ShiftScaleRotate(p=0.5),\n",
    "    RandomBrightnessContrast(contrast_limit=0.2, brightness_limit=0.2, p=0.5),\n",
    "    RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fd065-f59a-42b0-872e-4cac16386423",
   "metadata": {
    "id": "Ac98mnGJTxNu"
   },
   "source": [
    "La classe `Sequence` permet de définir l'accès aux données d'entraînement de manière personnalisée, afin par exemple d'implanter des augmentations particulières (c'est le cas ici).\n",
    "Prenez le temps de comprendre la synthaxe utilisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ca843-3309-4486-b8de-e8c0db82ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62541f0d-39b9-4651-a625-38c360a1b5ef",
   "metadata": {
    "id": "XaGczO0QT0H8"
   },
   "outputs": [],
   "source": [
    "class LSPDSequence(Sequence):\n",
    "    # Initialisation de la séquence avec différents paramètres\n",
    "    def __init__(self, x_set, y_set, batch_size,augmentations):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augmentations\n",
    "        self.indices1 = np.arange(x_set.shape[0]) \n",
    "        np.random.shuffle(self.indices1) # Les indices permettent d'accéder aux données \n",
    "        # et sont randomisés à chaque epoch pour varier la composition des batches au cours de l'entraînement\n",
    "\n",
    "        \n",
    "    # Fonction calculant le nombre de pas de descente du gradient par epoch\n",
    "    def __len__(self):\n",
    "        return int( np.ceil(len(self.x) / float(self.batch_size)) )\n",
    "\n",
    "    \n",
    "    # Application de l'augmentation de données à chaque image du batch et aux cartes de probabilités associées\n",
    "    def apply_augmentation(self, bx, by):\n",
    "\n",
    "        batch_x = np.zeros(bx.shape)\n",
    "        batch_y = np.zeros(by.shape)\n",
    "        # Pour chaque image du batch\n",
    "        for i in range(len(bx)):\n",
    "            masks = []\n",
    "            # Les 14 masques associés à l'image sont rangés dans une liste pour \n",
    "            # pourvoir être traités par la librairie Albumentation\n",
    "            for n in range(by.shape[3]):\n",
    "                masks.append(by[i,:,:,n])\n",
    "\n",
    "            img = bx[i]\n",
    "            # Application de l'augmentation à l'image et aux masques\n",
    "            transformed = self.augment(image=img, masks=masks)\n",
    "            batch_x[i] = transformed['image']\n",
    "            batch_y_list = transformed['masks']\n",
    "\n",
    "            # Reconstitution d'un tenseur à partir des masques augmentés\n",
    "            for k in range(by.shape[3]):\n",
    "                batch_y[i,:,:,k] = batch_y_list[k]\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    \n",
    "    # Fonction appelée à chaque nouveau batch : sélection et augmentation des données\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[self.indices1[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        batch_y = self.y[self.indices1[idx * self.batch_size:(idx + 1) * self.batch_size]]\n",
    "        \n",
    "        batch_x, batch_y = self.apply_augmentation(batch_x, batch_y)\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "    \n",
    "    # Fonction appelée à la fin d'un epoch ; on randomise les indices d'accès aux données\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa941a77-4490-4d1b-ad34-edfb6db4ff3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "NGBI2gXVT399",
    "outputId": "7ffaedf0-158e-4f1f-ae7a-75bcadc91ee8"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype(np.float32)\n",
    "y_hm_train = y_hm_train.astype(np.float32)\n",
    "\n",
    "# Instanciation d'une Sequence\n",
    "train_gen = LSPDSequence(x_train, y_hm_train, 16, augmentations=AUGMENTATIONS_TRAIN)\n",
    "\n",
    "# Pour tester la séquence, nous sélectionnons les éléments du premier batch et les affichons\n",
    "batch_x, batch_y = train_gen.__getitem__(0)\n",
    "\n",
    "print_heatmap(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537222b-e949-4cb9-900e-a14c9c99381d",
   "metadata": {
    "id": "yT-ySEuOUCyh"
   },
   "source": [
    "Pour utiliser cette séquence il vous suffit d'appeler la fonction `fit` de la manière suivante :\n",
    "```python\n",
    "model.fit(train_gen, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf66112-3b90-4fcc-8cfa-d3a92c5a77a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123f4f9-40ec-408c-8d3f-72c6a39d91a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510522b-4c3a-4520-9d59-a7f75d2dfe12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64008f84-8ff1-4fc0-95c3-a4b7089a233e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83940ed-3e9f-4352-9e4f-5c04a154beb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
