{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRcVJdNiFJ6Y"
   },
   "source": [
    "# Prédiction des prix de l'immobilier à Boston dans les années 1970"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJX3eu1lFSLZ"
   },
   "source": [
    "La prédiction du prix de maisons bostoniennes des années 1970, dont les données sont issues de la base *Boston House Prices*, créée par D. Harrison et D.L. Rubinfeld à l'Université de Californie à Irvine (http://archive.ics.uci.edu/ml/machine-learning-databases/housing/), est un problème classique d'apprentissage supervisé.\n",
    "\n",
    "<img src=\"https://1.bp.blogspot.com/-sCZIatDf9LQ/XGm-lEHXnAI/AAAAAAAAPxQ/kv8S8fdgudAwWTFuJhuAoiykLmWLCoOtgCLcBGAs/s1600/197010xx-GovernmentCenter-Boston_resize.JPG\" width=600 />\n",
    "\n",
    "Plus précisément, le label à prédire dans cette base de données est le prix médian par quartier de l'immobilier (en milliers de dollars). Il s'agit donc d'un problème de régression puisque l'on veut inférer des valeurs continues. Pour ce faire, on dispose de 13 entrées offrant les informations suivantes :\n",
    "\n",
    "- CRIM - per capita crime rate by town\n",
    "- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- INDUS - proportion of non-retail business acres per town.\n",
    "- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- NOX - nitric oxides concentration (parts per 10 million)\n",
    "- RM - average number of rooms per dwelling\n",
    "- AGE - proportion of owner-occupied units built prior to 1940\n",
    "- DIS - weighted distances to five Boston employment centres\n",
    "- TAX - full-value property-tax rate per \\$10,000\n",
    "- RAD - index of accessibility to radial highways\n",
    "- PTRATIO - pupil-teacher ratio by town\n",
    "- B $ = 1000(B_k - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "- LSTAT - percentage lower status of the population\n",
    "\n",
    "L'objectif de ce TP est d'arriver à prédire au plus près les valeurs médianes de prix de maison par quartier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSHfSdv8oxX-"
   },
   "source": [
    "![Texte alternatif…](https://miro.medium.com/max/763/1*i9vZk7NkS1dZz6JEcbV5nA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hy3k4aRnGl0r"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-j9idOyYyl-"
   },
   "source": [
    "##### _**Exercice** : Définir une fonction d'affichage `plot_loss` qui permet d'afficher erreur d'entraînement et de validation._\n",
    "\n",
    "On pourra tracer les courbes associées aux erreurs d'entraînement/de validation par epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/plot_loss.py\n",
    "def plot_loss(val_loss, train_loss, ymax=100):\n",
    "    plt.plot(val_loss, color='green', label='Erreur de validation')\n",
    "    plt.plot(train_loss, color='blue', linestyle='--', label='Erreur d\\'entraînement')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylim(0, ymax)\n",
    "    plt.title('Évolution de la perte sur les ensembles d\\'apprentissage et de validation au cours de l\\'apprentissage')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4HMTOhJRh9o"
   },
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D85ZzbYDY5Xs"
   },
   "source": [
    "On commence par charger les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gh8vgUo3859i",
    "outputId": "7199a7ce-9479-4bf9-a4f9-f1883becd77d"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJZfgaVwMGk9"
   },
   "source": [
    "## Approche simple à corriger\n",
    "\n",
    "Nous allons commencer par créer un perceptron multicouche élementaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1AKFeUnML3b"
   },
   "source": [
    "### Création du modèle\n",
    "\n",
    "La fonction `Sequential` permet d'instancier un réseau de neuronnes, la fonction `add` permet d'ajouter une couche au réseau, enfin la fonction `Dense` correspond à un perceptron (monocouche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQwBDvUq9VVn"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, activation='relu', input_dim=13))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ng3YnvV3RqVZ"
   },
   "source": [
    "### Entrainement du réseau\n",
    "\n",
    "La fonction `compile` permet de passer les arguments nécessaires à l'entraînement du réseau. `history` stocke les calculs de la loss pour chacune des epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtON8JXE9gj-",
    "outputId": "dd993ad2-06f0-43ff-b26e-ee719b7d101e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim = optimizers.SGD(lr = 0.01)\n",
    "model.compile(optimizer=optim, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTyU-K5jR1KA"
   },
   "source": [
    "### Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZdF2ihZPYnS",
    "outputId": "aff45989-e15e-4250-8f18-735c7e13d705"
   },
   "outputs": [],
   "source": [
    "train_loss=(history.history['loss'])\n",
    "plot_loss([], train_loss, ymax=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tzs6ZY3B-eut",
    "outputId": "13a2199c-cb59-472a-fdb0-cb387e3318f7"
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgbYzKDGYoaa"
   },
   "source": [
    "On obtient une mae d'environ 22, ce qui signie que l'on est éloigné en moyenne de 22000$ de la vérité terrain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire\n",
    "\n",
    "L'approche présentée ci-dessus apporte des résultats décevants, en raison de  quelques maladresses, voire erreurs. Dans un premier temps, vous devez **trouver et corriger ces problèmes**.\n",
    "\n",
    "Dans un second temps, cherchez à améliorer les performances du modèle. Vous pouvez atteindre sans trop de difficulté un score de MAE inférieur à 3 sur l'ensemble de test. A chaque nouveau test, vous devez évaluer si votre réseau est en sous-apprentissage, ou en sur-apprentissage, et en déduire des modifications possibles pour en améliorer les performances.\n",
    "\n",
    "MAE de test à battre si vous aimez les défis : **2.20** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction du modèle précédent\n",
    "\n",
    "Vous penserez à évaluer votre modèle à l'aide de la fonction `plot_loss` définie précédement et de la fonction `evaluate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/correction.py\n",
    "# Création d'un ensemble de validation\n",
    "(x, y), (x_test, y_test) = boston_housing.load_data()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=1/10, random_state=2)\n",
    "\n",
    "# Activation linéaire sur la couche de sortie\n",
    "model = Sequential()\n",
    "model.add(Dense(4, activation='relu', input_dim=13))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "optim = optimizers.Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer=optim, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Calcul de l'erreur de validation au cours de l'optimisation\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/evaluation.py\n",
    "# Evaluation du modèle\n",
    "train_loss=(history.history['mae'])\n",
    "val_loss=(history.history['val_mae'])\n",
    "plot_loss(val_loss, train_loss, ymax=30)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron monocouche\n",
    "\n",
    "Obtient-on des résultats comparables au réseau précédent avec un perceptron monocouche ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/monocouche.py\n",
    "model = Sequential()\n",
    "model.add(Dense(1, activation='linear', input_dim=13))\n",
    "\n",
    "optim = optimizers.Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer=optim, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle\n",
    "train_loss=(history.history['mae'])\n",
    "val_loss=(history.history['val_mae'])\n",
    "plot_loss(val_loss, train_loss, ymax=30)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propositions d'améliorations\n",
    "\n",
    "On peut certainement trouver de meilleures architectures !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amélioration n°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/amelioration_1.py\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=13))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "optim = optimizers.Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer=optim, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle\n",
    "train_loss=(history.history['mae'])\n",
    "val_loss=(history.history['val_mae'])\n",
    "plot_loss(val_loss, train_loss, ymax=30)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amélioration n°2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/amelioration_2.py\n",
    "(x, y), (x_test, y_test) = boston_housing.load_data()\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=1/10, random_state=2)\n",
    "\n",
    "# Normalisation des entrées\n",
    "x_mean = np.mean(x_train, axis=0)\n",
    "x_std = np.std(x_train, axis=0)\n",
    "\n",
    "x_train = (x_train-x_mean)/x_std\n",
    "x_val = (x_val-x_mean)/x_std\n",
    "x_test = (x_test-x_mean)/x_std\n",
    "\n",
    "print(x_train)\n",
    "print(x_std)\n",
    "\n",
    "\n",
    "# ------ #\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "# ------ #\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='relu', input_dim=13))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "\n",
    "optim = optimizers.Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer=optim, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle\n",
    "train_loss=(history.history['mae'])\n",
    "val_loss=(history.history['val_mae'])\n",
    "plot_loss(val_loss, train_loss, ymax=30)\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
